{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U bitsandbytes\n",
    "%pip install -U transformers\n",
    "%pip install -U accelerate\n",
    "%pip install -U peft\n",
    "%pip install -U trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding how Llama is initialised and Inferenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer\n",
    "import torch\n",
    "\n",
    "\n",
    "base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Who is the prime minister of India !\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a skilled Java developer specializing in data structures and algorithms.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you write me code to solve a two sum problem ?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=512, do_sample=True)\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "            outputs[0][\"generated_text\"].split(\n",
    "                \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "            )[1]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T21:35:59.264626Z",
     "iopub.status.busy": "2025-09-24T21:35:59.264286Z",
     "iopub.status.idle": "2025-09-24T21:36:09.920254Z",
     "shell.execute_reply": "2025-09-24T21:36:09.919252Z",
     "shell.execute_reply.started": "2025-09-24T21:35:59.264602Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 21:36:05.835459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758749765.859548     156 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758749765.866526     156 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T21:36:15.192292Z",
     "iopub.status.busy": "2025-09-24T21:36:15.191229Z",
     "iopub.status.idle": "2025-09-24T21:36:15.232069Z",
     "shell.execute_reply": "2025-09-24T21:36:15.231389Z",
     "shell.execute_reply.started": "2025-09-24T21:36:15.192263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Load Dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"/kaggle/input/synthetic-dataset/balanced_dataset.csv\")  # columns: description, category\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"category\"], random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T21:36:16.155080Z",
     "iopub.status.busy": "2025-09-24T21:36:16.154545Z",
     "iopub.status.idle": "2025-09-24T21:36:16.159813Z",
     "shell.execute_reply": "2025-09-24T21:36:16.159239Z",
     "shell.execute_reply.started": "2025-09-24T21:36:16.155056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Define Classes\n",
    "# -------------------------------\n",
    "labels = df[\"category\"].unique().tolist()\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T21:36:18.612348Z",
     "iopub.status.busy": "2025-09-24T21:36:18.611650Z",
     "iopub.status.idle": "2025-09-24T21:36:55.369298Z",
     "shell.execute_reply": "2025-09-24T21:36:55.368733Z",
     "shell.execute_reply.started": "2025-09-24T21:36:18.612323Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2900b34a1f4449b6926445a5bf4311c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 3. Load Tokenizer + Model\n",
    "# -------------------------------\n",
    "base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Quantization Config (4-bit)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,              # enable 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # computation dtype\n",
    "    bnb_4bit_use_double_quant=True,        # double quantization (saves memory)\n",
    "    bnb_4bit_quant_type=\"nf4\"              # normal float 4\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T21:37:08.625075Z",
     "iopub.status.busy": "2025-09-24T21:37:08.624778Z",
     "iopub.status.idle": "2025-09-24T21:37:09.041985Z",
     "shell.execute_reply": "2025-09-24T21:37:09.041123Z",
     "shell.execute_reply.started": "2025-09-24T21:37:08.625051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74c4db949474cc7aabcbfd5e33dc807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebaa6cc9cfb4739af1301d5f0a4550a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 4. Convert to Instruction Format\n",
    "# -------------------------------\n",
    "def format_example(example):\n",
    "    prompt = (\n",
    "        f\"Classify the following bank transaction into one of these categories:\\n\"\n",
    "        f\"{', '.join(labels)}\\n\\n\"\n",
    "        f\"Description: {example['description']}\\n\\nCategory:\"\n",
    "    )\n",
    "    target = f\" {example['category']}\"\n",
    "    return {\"text\": prompt + target}\n",
    "\n",
    "train_dataset = train_dataset.map(format_example)\n",
    "test_dataset = test_dataset.map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T21:37:12.986041Z",
     "iopub.status.busy": "2025-09-24T21:37:12.985303Z",
     "iopub.status.idle": "2025-09-24T21:37:14.292968Z",
     "shell.execute_reply": "2025-09-24T21:37:14.292244Z",
     "shell.execute_reply.started": "2025-09-24T21:37:12.986015Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd21d7df2c24a6ca721af9f404df842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd984bb39f844c6bb70d2dc79c3be09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 5. Tokenization\n",
    "# -------------------------------\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_fn, batched=True, remove_columns=train_dataset.column_names)\n",
    "test_tokenized = test_dataset.map(tokenize_fn, batched=True, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T21:37:17.355495Z",
     "iopub.status.busy": "2025-09-24T21:37:17.354937Z",
     "iopub.status.idle": "2025-09-24T21:37:17.503627Z",
     "shell.execute_reply": "2025-09-24T21:37:17.502818Z",
     "shell.execute_reply.started": "2025-09-24T21:37:17.355470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6. LoRA Config\n",
    "# -------------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T21:37:32.333135Z",
     "iopub.status.busy": "2025-09-24T21:37:32.332878Z",
     "iopub.status.idle": "2025-09-24T21:37:32.367787Z",
     "shell.execute_reply": "2025-09-24T21:37:32.367156Z",
     "shell.execute_reply.started": "2025-09-24T21:37:32.333117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 7. Training Setup\n",
    "# -------------------------------\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./llama-classifier\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",  # evaluate after each epoch\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T21:37:35.869681Z",
     "iopub.status.busy": "2025-09-24T21:37:35.869345Z",
     "iopub.status.idle": "2025-09-24T21:37:35.873486Z",
     "shell.execute_reply": "2025-09-24T21:37:35.872617Z",
     "shell.execute_reply.started": "2025-09-24T21:37:35.869660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 8. Data Collator\n",
    "# -------------------------------\n",
    "collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T23:04:55.769607Z",
     "iopub.status.busy": "2025-09-24T23:04:55.769025Z",
     "iopub.status.idle": "2025-09-24T23:04:55.776348Z",
     "shell.execute_reply": "2025-09-24T23:04:55.775652Z",
     "shell.execute_reply.started": "2025-09-24T23:04:55.769582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 9. Custom Evaluation\n",
    "# -------------------------------\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def evaluate_model(test_df):\n",
    "    y_true, y_pred = [], []\n",
    "    for _, row in test_df.iterrows():\n",
    "        prompt = (\n",
    "            f\"Classify the following bank transaction into one of these categories:\\n\"\n",
    "            f\"{', '.join(labels)}\\n\\n\"\n",
    "            f\"Description: {row['description']}\\n\\nCategory:\"\n",
    "        )\n",
    "        \n",
    "        # Generate\n",
    "        out = pipe(prompt, max_new_tokens=20, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "        # Try extracting after \"Category:\"\n",
    "        if \"Category:\" in out:\n",
    "            candidate = out.split(\"Category:\")[-1].strip()\n",
    "        else:\n",
    "            candidate = out  # fallback: whole output\n",
    "\n",
    "        # Take first word only (avoid extra sentences)\n",
    "        if candidate.strip():\n",
    "            pred = candidate.split()[0]\n",
    "        else:\n",
    "            pred = \"Miscellaneous\"  # fallback category\n",
    "\n",
    "        # Ensure prediction is a valid label\n",
    "        if pred not in labels:\n",
    "            # try to map by fuzzy match\n",
    "            matches = [lbl for lbl in labels if lbl.lower() in candidate.lower()]\n",
    "            pred = matches[0] if matches else \"Miscellaneous\"\n",
    "\n",
    "        y_true.append(row[\"category\"])\n",
    "        y_pred.append(pred)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, labels=labels))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred, labels=labels))\n",
    "    print(\"\\nAccuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T21:38:27.776309Z",
     "iopub.status.busy": "2025-09-24T21:38:27.776000Z",
     "iopub.status.idle": "2025-09-24T21:38:27.796949Z",
     "shell.execute_reply": "2025-09-24T21:38:27.796376Z",
     "shell.execute_reply.started": "2025-09-24T21:38:27.776287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 10. Trainer\n",
    "# -------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T21:38:40.695300Z",
     "iopub.status.busy": "2025-09-24T21:38:40.695036Z",
     "iopub.status.idle": "2025-09-24T22:59:51.190812Z",
     "shell.execute_reply": "2025-09-24T22:59:51.190039Z",
     "shell.execute_reply.started": "2025-09-24T21:38:40.695280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1128' max='1128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1128/1128 1:21:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.464400</td>\n",
       "      <td>0.448071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>0.402424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.394500</td>\n",
       "      <td>0.399548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1128, training_loss=0.5141635084828586, metrics={'train_runtime': 4870.0864, 'train_samples_per_second': 3.702, 'train_steps_per_second': 0.232, 'total_flos': 2.042263825931059e+16, 'train_loss': 0.5141635084828586, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 11. Train\n",
    "# -------------------------------\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T23:05:02.794203Z",
     "iopub.status.busy": "2025-09-24T23:05:02.793941Z",
     "iopub.status.idle": "2025-09-24T23:57:49.098370Z",
     "shell.execute_reply": "2025-09-24T23:57:49.097638Z",
     "shell.execute_reply.started": "2025-09-24T23:05:02.794183Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Education       1.00      0.35      0.52       125\n",
      "Travel & Transport       1.00      1.00      1.00       125\n",
      "         Groceries       0.95      1.00      0.98       125\n",
      "     Miscellaneous       0.42      0.90      0.58       125\n",
      " Bills & Utilities       1.00      1.00      1.00       125\n",
      "  Health & Fitness       1.00      1.00      1.00       125\n",
      "          Shopping       0.93      0.62      0.74       125\n",
      "     Entertainment       0.83      0.77      0.80       125\n",
      "       Investments       0.85      0.75      0.80       126\n",
      "            Income       1.00      0.97      0.98       126\n",
      "     Food & Drinks       0.99      0.97      0.98       126\n",
      "       Withdrawals       0.95      1.00      0.97       125\n",
      "\n",
      "          accuracy                           0.86      1503\n",
      "         macro avg       0.91      0.86      0.86      1503\n",
      "      weighted avg       0.91      0.86      0.86      1503\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 44   0   6  54   0   0   3  16   1   0   1   0]\n",
      " [  0 125   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 125   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 113   0   0   0   2   3   0   0   7]\n",
      " [  0   0   0   0 125   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 125   0   0   0   0   0   0]\n",
      " [  0   0   0  42   0   0  77   2   4   0   0   0]\n",
      " [  0   0   0  18   0   0   3  96   8   0   0   0]\n",
      " [  0   0   0  32   0   0   0   0  94   0   0   0]\n",
      " [  0   0   0   4   0   0   0   0   0 122   0   0]\n",
      " [  0   0   0   4   0   0   0   0   0   0 122   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 125]]\n",
      "\n",
      "Accuracy: 0.8602794411177644\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 12. Final Evaluation\n",
    "# -------------------------------\n",
    "evaluate_model(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T23:57:49.099698Z",
     "iopub.status.busy": "2025-09-24T23:57:49.099481Z",
     "iopub.status.idle": "2025-09-24T23:57:49.373431Z",
     "shell.execute_reply": "2025-09-24T23:57:49.372862Z",
     "shell.execute_reply.started": "2025-09-24T23:57:49.099681Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llama-classifier/tokenizer_config.json',\n",
       " './llama-classifier/special_tokens_map.json',\n",
       " './llama-classifier/chat_template.jinja',\n",
       " './llama-classifier/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 13. Save\n",
    "# -------------------------------\n",
    "model.save_pretrained(\"./llama-classifier\")\n",
    "tokenizer.save_pretrained(\"./llama-classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing Finetuned-Llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T00:35:11.523666Z",
     "iopub.status.busy": "2025-09-25T00:35:11.523017Z",
     "iopub.status.idle": "2025-09-25T00:35:20.043985Z",
     "shell.execute_reply": "2025-09-25T00:35:20.043331Z",
     "shell.execute_reply.started": "2025-09-25T00:35:11.523640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9552299c514407a831b7180212ad3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food & Drinks\n",
      "Investments\n",
      "Income\n",
      "Bills & Utilities\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"./llama-classifier\",\n",
    "    tokenizer=\"./llama-classifier\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def classify_transaction(text):\n",
    "    prompt = (\n",
    "        f\"Classify the following bank transaction into one of these categories:\\n\"\n",
    "        f\"{', '.join(labels)}\\n\\n\"\n",
    "        f\"Description: {text}\\n\\nCategory:\"\n",
    "    )\n",
    "    output = pipe(prompt, max_new_tokens=20, do_sample=False)\n",
    "    generated = output[0][\"generated_text\"].split(\"Category:\")[-1].strip().split(\"\\n\")[0].strip()\n",
    "\n",
    "    return generated  # fallback\n",
    "\n",
    "\n",
    "print(classify_transaction(\"Sent Rs.510.00 From HDFC Bank A/C *0552 To Swiggy Limited On 20/09/25 Ref 111495595089 Not You? Call 18002586161/SMS BLOCK UPI to 7308080808\"))\n",
    "print(classify_transaction(\"UPI Payment to Amazon\"))\n",
    "print(classify_transaction(\"Salary credited from company\"))\n",
    "print(classify_transaction(\"Recharge of Airtel mobile\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing into Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T00:03:21.227597Z",
     "iopub.status.busy": "2025-09-25T00:03:21.227006Z",
     "iopub.status.idle": "2025-09-25T00:03:24.323355Z",
     "shell.execute_reply": "2025-09-25T00:03:24.322421Z",
     "shell.execute_reply.started": "2025-09-25T00:03:21.227571Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T00:03:52.776773Z",
     "iopub.status.busy": "2025-09-25T00:03:52.776451Z",
     "iopub.status.idle": "2025-09-25T00:03:52.926633Z",
     "shell.execute_reply": "2025-09-25T00:03:52.925890Z",
     "shell.execute_reply.started": "2025-09-25T00:03:52.776740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "login(os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T00:24:40.861040Z",
     "iopub.status.busy": "2025-09-25T00:24:40.860728Z",
     "iopub.status.idle": "2025-09-25T00:24:45.785150Z",
     "shell.execute_reply": "2025-09-25T00:24:45.784377Z",
     "shell.execute_reply.started": "2025-09-25T00:24:40.861010Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44772a37ffb445b6bf703f02aa4d61ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20d27f6b5514f8da89599f0e7713084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/karthiksagarn/llama3-3.2b-finetuned-financial/commit/98dac881f092e17ee2dd44d05ea3bf0cf72cdab2', commit_message='Upload folder using huggingface_hub', commit_description='', oid='98dac881f092e17ee2dd44d05ea3bf0cf72cdab2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/karthiksagarn/llama3-3.2b-finetuned-financial', endpoint='https://huggingface.co', repo_type='model', repo_id='karthiksagarn/llama3-3.2b-finetuned-financial'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"./llama-classifier\",\n",
    "    repo_id=\"karthiksagarn/llama3-3.2b-finetuned-financial\",\n",
    "    repo_type=\"model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8340377,
     "sourceId": 13162702,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 121027,
     "modelInstanceId": 100936,
     "sourceId": 120005,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
